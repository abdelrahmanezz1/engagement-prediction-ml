\# ğŸ¬ Engagement Prediction from Video \& Audio



This project builds a machine learning pipeline to predict \*\*user engagement\*\* from short video recordings using \*\*audio + video features\*\*.



---



\## ğŸš€ What this project does



\- Splits videos into labeled segments (high, mid, low, read, talk, idle)

\- Extracts features:

&nbsp; - ğŸ§ Audio (MFCCs, spectral, ZCR, chroma via Librosa)

&nbsp; - ğŸ§  Meta info (segment type, duration)

\- Trains ML models to predict engagement:

&nbsp; - 3-class: \*\*low / mid / high\*\*

&nbsp; - Binary: \*\*high vs non-high\*\* (best)



---



\## ğŸ“Š Results (Stratified 5-Fold CV)



| Task                     | Best Model     | Accuracy | Macro F1 |

|--------------------------|----------------|----------|----------|

| 3-Class (low/mid/high)   | Random Forest  | \*\*61.4%\*\* | 0.42     |

| Binary (high vs non-high)| XGBoost        | \*\*65.7%\*\* | \*\*0.62\*\* |



â¡ Binary works better because \*\*â€œmidâ€ engagement is subjective\*\* and hard to learn with few samples.



---



\## ğŸ—‚ Project structure



```text

notebooks/

â”œâ”€â”€ 01-feature-extraction.ipynb          # Extract audio + video features

â”œâ”€â”€ 02-training-evaluation-multiclass.ipynb

â””â”€â”€ 03-training-evaluation-binary.ipynb  # Best-performing model





ğŸ” Key points



&nbsp;-Small real dataset: 6 subjects, ~34 labeled segments



&nbsp;-Engagement labels are self-reported  (1â€“5)



&nbsp;-Smart reframing: from noisy 3-class â†’  more reliable binary



&nbsp;-Shows full pipeline: data â†’ features â†’ models â†’ evaluation



ğŸ§ªTech stack



Python Â· Scikit-Learn Â· XGBoost Â· Librosa Â· OpenCV Â· PCA Â· Jupyter

